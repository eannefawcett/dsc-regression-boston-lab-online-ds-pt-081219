{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Regression Modeling with the Boston Housing Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this final lab, you'll apply the regression analysis and diagnostics techniques covered in this section to the famous \"Boston Housing\" dataset. You performed a detailed EDA for this dataset earlier on, and hopefully, you more or less recall how this data is structured! In this lab, you'll use some of the features in this dataset to create a linear model to predict the house price!\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Build many linear models with the Boston housing data using OLS\n",
    "* Analyze OLS diagnostics for model validity \n",
    "* Visually explain the results and interpret the diagnostics from Statsmodels \n",
    "* Comment on the goodness of fit for a simple regression model\n",
    "\n",
    "## Let's get started\n",
    "\n",
    "### Import necessary libraries and load 'BostonHousing.csv' as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "boston_df['Target'] = pd.Series(boston.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the Boston housing data represent the dependent and independent variables. The dependent variable here is the median house value `MEDV`. The description of the other variables is available on [KAGGLE](https://www.kaggle.com/c/boston-housing). \n",
    "\n",
    "### Inspect the columns of the dataset and comment on type of variables present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "boston_df.head()\n",
    "boston_df.describe()\n",
    "# boston_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Record your observations here \n",
    "# CRIM has a heavy left skew, looking at the quartiles\n",
    "# ZN does as well, with the bulk of the data being zeros\n",
    "# INDUS has more varied data than CRIM and ZN, but the mean is higher than the median, skewed right\n",
    "# CHAS is not going to be a use category\n",
    "# NOX has varied data, with the mean and the median being almost equal -- possible coorelation here\n",
    "# RM has varied data, with the mean and the median being almost equal -- possible coorelation here\n",
    "# AGE has varied data, where the mean is less than the median, so there is a skew to the left\n",
    "# DIS has varied data, with the mean and median being almost equal -- possible coorelation\n",
    "# RAD is not going to be a useful category - categorical in nature\n",
    "# TAX is not going to be a useful category - categorical in nature\n",
    "# PTRATIO has varied data, with the mean and median being almost equal -- possible coorelation\n",
    "# B has varied data, where the mean is less than the median, so there is a skew to the left\n",
    "# LSTAT has varied data, with the mean and median being almost equal -- possible coorelation\n",
    "# TARGET has varied data, with the mean and median being almost equal -- possible correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create histograms for all variables in the dataset and comment on their shape (uniform or not?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "boston_df.hist(figsize=(15,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You observations here \n",
    "# Age is skewed right\n",
    "# B is garbage\n",
    "# CHAS is garbage\n",
    "# CRIM is skewed left\n",
    "# DIS is skewed left\n",
    "# INDUS is missing the middle\n",
    "# LSTAT is skewed left\n",
    "# NOX is skewed left\n",
    "# PTRATIo looks promising\n",
    "# RAD is garbage\n",
    "# RM looks great, if a little tall\n",
    "# TAX looks like garbage, possible outliers\n",
    "# TARGET look great!\n",
    "# ZN is skewed left - maybe some 0 values that are supposed to be n/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we preselected some features  for you which appear to be more 'normal' than others.\n",
    "### Create a new dataset with `['crim', 'dis', 'rm', 'zn', 'age', 'medv']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "frames = [boston_df['CRIM'], boston_df['DIS'], boston_df['RM'], boston_df['ZN'], boston_df['AGE'], boston_df['Target']]\n",
    "boston_df_more_normal = pd.concat(frames, axis=1, join='outer', sort=True)\n",
    "original_columns = boston_df_more_normal.columns\n",
    "new_columns = ['crim', 'dis', 'rm', 'zn', 'age', 'medv']\n",
    "boston_df_more_normal.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for linearity assumption for all chosen features with target variable using scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "y = boston_df_more_normal['medv']\n",
    "# x_array = np.array(x)\n",
    "# plot_x = pd.DataFrame([x_array]*6).transpose()\n",
    "# plot_x.columns = ['medv', 'medv', 'medv', 'medv', 'medv', 'medv']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, squeeze=False)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "# for j in range(2):\n",
    "#     for k in range(3):\n",
    "#         y = pd.DataFrame([boston_df_more_normal[n] for n in boston_df_more_normal.columns])\n",
    "#         plot_y = pd.DataFrame([y[i] for i in y.columns])\n",
    "#         ax[j, k].scatter(plot_x,plot_y)\n",
    "# for n in range(1,6):\n",
    "#     row = (n-1)//3\n",
    "#     col = n%3-1\n",
    "#     ax = axes[row][col]\n",
    "#     ax.scatter(plot_x,plot_y)\n",
    "# print(x_arrays)\n",
    "# print(plot_y)\n",
    "x = pd.DataFrame([boston_df_more_normal[n] for n in boston_df_more_normal.columns])\n",
    "plot_x = pd.DataFrame([x[i] for i in x.columns])\n",
    "\n",
    "axes[0,0].scatter(plot_x['crim'], y)\n",
    "axes[0,0].set_title('crim vs mdev')\n",
    "axes[0,1].scatter(plot_x['dis'], y)\n",
    "axes[0,1].set_title('dis vs mdev')\n",
    "axes[0,2].scatter(plot_x['rm'], y)\n",
    "axes[0,2].set_title('rm vs mdev')\n",
    "axes[1,0].scatter(plot_x['zn'], y)\n",
    "axes[1,0].set_title('zn vs mdev')\n",
    "axes[1,1].scatter(plot_x['age'], y)\n",
    "axes[1,1].set_title('age vs mdev');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your observations here \n",
    "# crim and zn coult maybe use a log transformation\n",
    "# rm is a really good candidate for linear regression already\n",
    "# dis and age could use some work, but I don't know what that would look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, your data needs a lot of preprocessing to improve the results. This key behind a Kaggle competition is to process the data in such a way that you can identify the relationships and make predictions in the best possible way. For now, we'll the dataset untouched and just move on with the regression. The assumptions are _exactly_ all fulfilled, but they still hold to a level that we can move on. \n",
    "\n",
    "### Let's do Regression \n",
    "\n",
    "Now, let's perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). You'll do this in a loop and in every iteration, you should pick one of the independent variables. Perform the following steps:\n",
    "\n",
    "* Run a simple OLS regression between independent and dependent variables\n",
    "* Plot a regression line on the scatter plots\n",
    "* Plot the residuals using `sm.graphics.plot_regress_exog()`\n",
    "* Plot a Q-Q plot for regression residuals normality test \n",
    "* Store following values in array for each iteration:\n",
    "    * Independent Variable\n",
    "    * r_squared'\n",
    "    * intercept'\n",
    "    * 'slope'\n",
    "    * 'p-value'\n",
    "    * 'normality (JB)' \n",
    "* Comment on each output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "# build the formula \n",
    "# f1 = 'mdev~crim'\n",
    "# f2 = 'mdev~dis'\n",
    "# f3 = 'mdev~rm'\n",
    "# f4 = 'mdev~zn'\n",
    "# f5 = 'mdev~age'\n",
    "# # create a fitted model in one line\n",
    "# model1 = ols(formula=f1, data=boston_df_more_normal).fit()\n",
    "# model2 = ols(formula=f2, data=boston_df_more_normal).fit()\n",
    "# model3 = ols(formula=f3, data=boston_df_more_normal).fit()\n",
    "# model4 = ols(formula=f4, data=boston_df_more_normal).fit()\n",
    "# model5 = ols(formula=f5, data=boston_df_more_normal).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ind_var</td>\n",
       "      <td>r_squared</td>\n",
       "      <td>intercept</td>\n",
       "      <td>slope</td>\n",
       "      <td>p-value</td>\n",
       "      <td>normality (JB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crim</td>\n",
       "      <td>0.15078</td>\n",
       "      <td>24.0331</td>\n",
       "      <td>-0.41519</td>\n",
       "      <td>1.17399e-19</td>\n",
       "      <td>295.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dis</td>\n",
       "      <td>0.0624644</td>\n",
       "      <td>18.3901</td>\n",
       "      <td>1.09161</td>\n",
       "      <td>1.20661e-08</td>\n",
       "      <td>305.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rm</td>\n",
       "      <td>0.483525</td>\n",
       "      <td>-34.6706</td>\n",
       "      <td>9.10211</td>\n",
       "      <td>2.48723e-74</td>\n",
       "      <td>612.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zn</td>\n",
       "      <td>0.129921</td>\n",
       "      <td>20.9176</td>\n",
       "      <td>0.14214</td>\n",
       "      <td>5.71358e-17</td>\n",
       "      <td>262.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>age</td>\n",
       "      <td>0.142095</td>\n",
       "      <td>30.9787</td>\n",
       "      <td>-0.123163</td>\n",
       "      <td>1.56998e-18</td>\n",
       "      <td>456.983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2         3            4               5\n",
       "0  ind_var  r_squared  intercept     slope      p-value  normality (JB)\n",
       "1     crim    0.15078    24.0331  -0.41519  1.17399e-19         295.404\n",
       "2      dis  0.0624644    18.3901   1.09161  1.20661e-08         305.104\n",
       "3       rm   0.483525   -34.6706   9.10211  2.48723e-74         612.449\n",
       "4       zn   0.129921    20.9176   0.14214  5.71358e-17         262.387\n",
       "5      age   0.142095    30.9787 -0.123163  1.56998e-18         456.983"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your observations here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the results are not very reliable. The best R-Squared is witnessed with `rm`, so in this analysis, this is uour best predictor. \n",
    "\n",
    "### How can you improve these results?\n",
    "1. Preprocessing \n",
    "\n",
    "This is where preprocessing of data comes in. Dealing with outliers, normalizing data, scaling values etc. can help regression analysis get more meaningful results from the given data. \n",
    "\n",
    "2. Advanced Analytical Methods\n",
    "\n",
    "Simple regression is a very basic analysis technique and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. Later on, you'll explore at multiple regression where you can use multiple features **at once** to define a relationship with the outcome. You'll also look at some preprocessing and data simplification techniques and revisit the Boston dataset with an improved toolkit. \n",
    "\n",
    "## Level up - Optional \n",
    "\n",
    "Apply some data wrangling skills that you have learned in the previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the goodness of fit. \n",
    "\n",
    "## Summary \n",
    "\n",
    "In this lab, you applied your skills learned so far on a new data set. You looked at the outcome of your analysis and realized that the data might need some preprocessing to see a clear improvement in results. You'll pick this back up later on, after learning about more preprocessing techniques and advanced modeling techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
